{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO7vFQCczxqS+PL1eD+12DH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fmYM4VkPcSDk"},"outputs":[],"source":["!pip install -q --upgrade bitsandbytes accelerate"]},{"cell_type":"code","source":["from google.colab import userdata\n","from huggingface_hub import login\n","from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n","import torch\n","import gc"],"metadata":{"id":"XAFYHInFgYrI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hf_token = userdata.get('HuggingFace')\n","login(hf_token, add_to_git_credential=True)"],"metadata":{"id":"DtOjLJZ6gjNB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PHI = \"microsoft/Phi-4-mini-instruct\"\n","QWEN = \"Qwen/Qwen3-4B-Instruct-2507\"\n","DEEPSEEK = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\""],"metadata":{"id":"dEFQvDd8gquk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["messages = [\n","    {\"role\": \"user\", \"content\": \"Tell a joke for a room of Data Scientists\"}\n","  ]"],"metadata":{"id":"od1dCVlKg7R8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Quantization Config - this allows us to load the model into memory and use less memory\n","\n","quant_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n","    bnb_4bit_quant_type=\"nf4\"\n",")"],"metadata":{"id":"yeFvdXq3g8Ft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(QWEN)\n","tokenizer.pad_token = tokenizer.eos_token\n","inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"],"metadata":{"id":"WSBu0C0thWTd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs"],"metadata":{"id":"4EaGSMY4hyhy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The model\n","\n","model = AutoModelForCausalLM.from_pretrained(QWEN, device_map=\"auto\", quantization_config=quant_config)"],"metadata":{"id":"MIywhR8fh1Ur"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["memory = model.get_memory_footprint() / 1e6\n","print(f\"Memory footprint: {memory:,.1f} MB\")"],"metadata":{"id":"FCMcN_AVh6ge"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Execute this cell and look at what gets printed; investigate the layers\n","\n","model"],"metadata":{"id":"6HC2cHvJjOqa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# OK, with that, now let's run the model!\n","\n","outputs = model.generate(inputs, max_new_tokens=80)\n","outputs[0]"],"metadata":{"id":"bEwAOQmbk9DX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Well that doesn't make much sense!\n","# How about this..\n","\n","tokenizer.decode(outputs[0])"],"metadata":{"id":"tvhx3KeulD6n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["del model, inputs, tokenizer, outputs\n","gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"0iccMFSKlq4E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate(model, messages, quant=True, max_new_tokens=80):\n","  tokenizer = AutoTokenizer.from_pretrained(model)\n","  tokenizer.pad_token = tokenizer.eos_token\n","  input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n","  attention_mask = torch.ones_like(input_ids, dtype=torch.long, device=\"cuda\")\n","  streamer = TextStreamer(tokenizer)\n","  if quant:\n","    model = AutoModelForCausalLM.from_pretrained(model, quantization_config=quant_config).to(\"cuda\")\n","  else:\n","    model = AutoModelForCausalLM.from_pretrained(model).to(\"cuda\")\n","  outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, streamer=streamer)"],"metadata":{"id":"_xyoENHvlxHv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generate(PHI, messages)"],"metadata":{"id":"5Qa4gbkzlzfA"},"execution_count":null,"outputs":[]}]}